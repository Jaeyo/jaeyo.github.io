<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FlowerBud</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 28 Apr 2017 17:54:35 +0900</pubDate>
    <lastBuildDate>Fri, 28 Apr 2017 17:54:35 +0900</lastBuildDate>
    <generator>Jekyll v3.3.1</generator>
    
      <item>
        <title>mariadb multi source replication</title>
        <description>&lt;h2 id=&quot;multi-source-replication-절차&quot;&gt;multi source replication 절차&lt;/h2&gt;
&lt;p&gt;slave가 될 mysql 인스턴스의 설정에 아래의 부분을 추가한다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# /etc/mysql/maridb.conf.d/50-server.cnf&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;mysqld]
&lt;span class=&quot;nv&quot;&gt;server_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;19823759 &lt;span class=&quot;c&quot;&gt;# master와 겹치지 않게&lt;/span&gt;
replicate-ignore-db&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mysql &lt;span class=&quot;c&quot;&gt;# mysql 데이터베이스는 복제하지 않음&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;master 데이터베이스에서 replication 용 계정 생성 후 권한을 부여한다&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;grant replication slave on &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; to ‘repl’@‘%’ identified by ‘!repl123’;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;mysqldump 를 이용해서 master가 될 데이터베이스에서 덤프를 받는다.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysqldump --databases database_1 database_2 --single_transaction --master-data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 -h host_path -u account -p &amp;gt; dump.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;락이 걸리는 걸 방지하기 위해 &lt;code class=&quot;highlighter-rouge&quot;&gt;—single-transaction&lt;/code&gt; 옵션 추가해야 하며, 덤프를 받은 시점의 binlog 파일과 포지션을 확인하기 위해 &lt;code class=&quot;highlighter-rouge&quot;&gt;—master-data=1&lt;/code&gt; 옵션 추가한다. 그러면 덤프 파일 상단에 &lt;code class=&quot;highlighter-rouge&quot;&gt;change master … &lt;/code&gt; 구문이 생성되는데 multi source 로부터 replication 을 받아야 하기 때문에 아래와 같이 수정해준다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CHANGE MASTER &lt;span class=&quot;s1&quot;&gt;'master_name'&lt;/span&gt; TO
&lt;span class=&quot;nv&quot;&gt;master_host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'host_path'&lt;/span&gt;,
&lt;span class=&quot;nv&quot;&gt;master_user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'account'&lt;/span&gt;,
&lt;span class=&quot;nv&quot;&gt;master_password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'password'&lt;/span&gt;,
&lt;span class=&quot;nv&quot;&gt;master_port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3306,
&lt;span class=&quot;nv&quot;&gt;MASTER_LOG_FILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'mysql-bin-changelog.052616'&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;MASTER_LOG_POS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;886626; &lt;span class=&quot;c&quot;&gt;# dump에 기록된 그대로 유지&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;만약 덤프 파일이 너무 커서 수정하기가 힘든 경우 아래와 같이 파일을 잘라서 수정한 후에 합치는 방법을 사용할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;head -n 30 dump.sql &amp;gt; new_dump.sql
vim new_dump.sql
tail -n +30 dump.sql &amp;gt;&amp;gt; new_dump.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;slave가 될 데이터베이스에 덤프를 밀어넣는다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysql -h hostpath -u account -p database_name &amp;lt; dump.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;여기서 database_name은 덤프 내 여러개의 데이터베이스가 담겨 있다고 해도 그 중 하나만 지정해도 되지만 지정된 데이터베이스는 미리 생성되어 있어야 한다.&lt;/p&gt;

&lt;p&gt;replication 스레드를 시작시킨다:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slave&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'master_name'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;기타-유의사항&quot;&gt;기타 유의사항&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;replication 설정을 위해서는 super previleges가 필요한데 aws rds를 이용하면 일반 사용자가 super previleges를 가질 수 없다.&lt;/li&gt;
  &lt;li&gt;binary log가 너무 빨리 rotate 되는 경우에는 아래와 같이 retention 값을 확인 및 조정할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;call mysql.rds_show_configuration()&lt;/code&gt; 으로 &lt;code class=&quot;highlighter-rouge&quot;&gt;binlog retention hours&lt;/code&gt; 값 확인
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;call mysql.rds_set_configuration(‘binlog retention hours’, 1);&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 29 Apr 2017 02:51:00 +0900</pubDate>
        <link>http://localhost:4000/tech/2017/04/29/mysql-multi-source-replication.html</link>
        <guid isPermaLink="true">http://localhost:4000/tech/2017/04/29/mysql-multi-source-replication.html</guid>
        
        
        <category>tech</category>
        
      </item>
    
      <item>
        <title>Too many open files</title>
        <description>&lt;p&gt;집에서 기타치면서 놀고 있었는데 외출했던 아내가 귀가중이라는 메시지를 받았다. 집 앞 지하척 역으로 마중나가려고 핸드폰을 챙겨서 나가려는데 슬랙 메시지가 뭐가 많이 와있다. 평소 팀에서 커뮤니케이션을 슬랙으로 하기 때문에 별 생각없이 슬랙 메시지들을 확인해보고 있는데 서버쪽에 장애가 날 때 메시지가 오는 채널에 몇개의 메시지가 와있다. 순간 심장이 덜컹하며 확인해보니 몇가지 종류의 에러메시지가 꽤 여러번 발생한 형태다.&lt;/p&gt;

&lt;p&gt;첫번째 에러는 too many open files
두번째 에러는 unable to find server blabla&lt;/p&gt;

&lt;p&gt;두번째 에러로그는 심지어 디비서버를 못찾는다는 에러메시지!
디비 서버를 못찾았다면 서비스 자체가 안될텐데.
마음을 졸이며 우리 서비스에 들어가보니 일단 접속도 잘 되고 데이터 로딩도 문제없이 된다. 그렇다면 사태는 일단락 되었다는 이야긴데..&lt;/p&gt;

&lt;p&gt;그렇다고 손놓고 다시 기타나 뚱기뚱기할 수는 없으니 다시 로그를 뒤진다. was 로그와 syslog 등등을 종합적으로 뒤지고 수상해보이는 놈들은 죄 구글링을 해봤지만 별게 안나왔다.
할 수 없이 에러로그에 집중해보니, 일단 too many open files가 먼저 나고 그 다음에 디비서버를 못찾았다. 혹시나 싶어 로드밸런서에 물려있는 다른 서버를 확인해봤는데 다른 서버는 문제가 없었다. 그렇다면 실제 디비서버가 나가리가 되었다거나 한 문제는 아니라고 볼 수 있겠다고 생각했다.&lt;/p&gt;

&lt;p&gt;일단 too many open files가 떠있으니 ulimit 을 확인해주는데, 어라? 1024? 왜 이렇게 값이 작지?&lt;/p&gt;

&lt;p&gt;찬찬히 머리속에서 히스토리들을 뒤져보는데 한달전쯤 was를 변경했고 was가 변경되면서 리눅스 계정이 바꼈고 바뀐 리눅스 계정에 대해서 ulimit 값을 변경….을 안해줬구나. 아…&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Apr 2017 04:25:01 +0900</pubDate>
        <link>http://localhost:4000/troubleshoot/2017/04/26/too-many-open-files.html</link>
        <guid isPermaLink="true">http://localhost:4000/troubleshoot/2017/04/26/too-many-open-files.html</guid>
        
        
        <category>troubleshoot</category>
        
      </item>
    
      <item>
        <title>아닌 밤중에 리퀘스트가 튄다?</title>
        <description>&lt;p&gt;신규 서비스를 오픈하면서 서버 모니터링을 위해 aws cloudwatch를 사용하게 되었다. 원래는 cloudwatch와 newrelic 을 모두 선택지에 올려놓고 마지막까지 고민을 했다. newrelic 쪽이 기능도 많고 레퍼런스도 확실했지만 일부 서버에서 agent의 설치가 안되는 문제가 있었다. 사실 agent 설치 안되는 문제야 조금 더 시간을 들여서 파보면 해결할 수 있는 문제였겠지만 당장에 서비스 오픈을 코앞에 둔 상황에서 시간적인 여유가 부족했고, cloudwatch도 우리가 필요한 기능을 대부분 제공하고 있었기 때문에 cloudwatch로 가기로 했다.&lt;/p&gt;

&lt;p&gt;cpu, 메모리, 디스크 사용량 등의 지표를 대시보드로 구성해놓고 며칠 모니터링하고 있으려니까 좀 이상한 현상이 눈에 띄었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/H99gBeu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일부 서버에서 주기적으로 network out이 튀고 있었다. 시간도 정확하게 6시 반쯤에 시작해서 11시에는 정상으로 돌아왔다. 제일 먼저 해당 서버에서 주기적으로 스케쥴링되어 돌고 있는 태스크가 있는지 확인했다. 하지만 스케쥴이 걸려있는 태스크라고는 cloudwatch 메트릭 값을 수집하는 agent 뿐이었고 이 에이전트도 5분마다 한번씩 실행되도록 구성되어 있었기 때문에 용의선상에서 벗어났다. 그외에 여러 의심되는 부분들을 샅샅이 뒤져봤으나 애초에 해당 서버는 aws s3에 있는 데이터들을 변환해 던져주는 역할만 하는 심플한 서버였기 때문에 원인을 진단하기가 어려웠다. network out이 튀는 정도가 분당 평균 5mb 근처였기 때문에 당장 서비스 운영에 문제를 줄만한 사안이 아니었고 이 문제 말고도 긴급을 요하는 태스크들이 많았기 때문에 자연히 칸반 보드의 todo list 어딘가로 해당 이슈는 쳐박히게 되었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/RPkrl0O.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;문제는 다른 부분에서 다시 일어났다. 이 서버의 메모리 사용량이 슬금슬금 눈치채지 못할 정도로 느리게 증가하고 있었다. 최초 기동시에는 6퍼센트 정도에서 머물던 것이 시간이 지남에 따라 50%를 넘어 70%를 넘보는 수준까지 도달한 것이었다. 그때 팀원 중 한명이 s3fs를 의심하기 시작했다. 이 서버는 s3에 있는 데이터를 서빙하기 위해서 s3fs 라는 오픈소스를 이용해서 s3 특정 버킷을 가상으로 마운트 시켜놓고 쓰고 있었다. s3fs 프로젝트의 issue란을 뒤져보니 memory leak이 있다는 issue가 유난히 많았다. 팀 내부에서도 s3fs의 memory leak bug로 인한 현상으로 추정하고 옵션값 튜닝을 더 해보고 안되면 s3fs 대신 goofys 라는 솔루션으로 넘어가기로 했다. 하지만 그대로 s3fs의 memory leak bug 라고 단정해버리기엔 좀 찜찜한 것이, github issue에 올라온 memory leak 리포트들은 대부분 수십분에서 수 시간 내로 서버의 메모리를 모두 잡아먹고 뻗어버리는 현상이 발생했던 것과는 달리 우리 서버는 2주째 메모리 사용량을 야금야금 먹으면서 올라오긴 했지만 서버의 메모리를 모두 잡아먹진 않았다. 조금 더 살펴보기로 했다.&lt;/p&gt;

&lt;p&gt;일단 좀 더 정확한 정황을 확인하기 위해 s3fs의 디버그 로그 옵션을 켜고 다시 재기동하였다. syslog에 어마어마한 양의 로그가 쌓이기 시작했다. 양 자체가 워낙에 많에 전수조사를 할 수는 없었지만 일단 일부 확인한 내용으로는 이상해보이는 로그는 보이지 않았다. 그렇게 주말을 보내고 월요일 아침 다시 로그를 확인해보았다. 이 때 였다. 의심스러운 로그가 눈에 들어오기 시작했다.&lt;/p&gt;

&lt;p&gt;매일 아침 6시 25분마다 s3fs에 의해서 가상 마운트되어 있는 폴더 아래 모든 오브젝트들을 하나씩 순회하는 로그들이 찍혀있었다. 해당 s3 bucket에는 수만개 이상의 오브젝트들이 존재하고 있기 때문에 이 순회 작업은 수시간 후에야 끝났다. 그 때 잊고 있었던 네트워크 튐 현상이 떠올랐다. network out 지표에도 매일 아침 6시 반부터 11시 까지 그래프가 튀어 있었고 s3fs 로그에서는 그 시간대에 bucket내 모든 오브젝트들을 순회하는 로그가 찍혀있었다. 즉, s3fs 마운트 폴더에서의 순회요청은 그대로 디스크를 뒤지는 것이 아니라 network를 타고 나가 s3 서버로 요청을 던지게 되므로 network out 그래프가 튀게 된 것이다. 범인은 꼬리가 보이기 시작했다. 우리는 이 단서를 조금 더 파보기 시작했다.&lt;/p&gt;

&lt;p&gt;한가지 더 이상했던 것은 s3fs 마운트 폴더를 순회하는 로그가 정확히 각 syslog의 가장 첫부분부터 시작이 되고 있었다는 것이다. syslog는 logrotate에 의해서 롤링되고 있었기 때문에 logrotate가 실행되는 시점에 어떤 명령에 의해서 s3fs 마운트 폴더 순회 명령이 걸렸다고 추정할 수 있었다. 범인의 몽타주가 얼핏 보이는 듯 했다. logrotate 실행 스크립트가 담겨있는 cron.daily 폴더를 조회해보았다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/t2dXBN8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;운좋게 몇놈 없었다. 이 놈들 중 범인이 있다고 확신한 나는 각 스크립트들을 하나씩 취조하기 시작했고 마침내 mlocate가 범인임을 확인할 수 있었다. mlocate는 빠른 파일 접근을 위해 디스크 내용을 인덱싱하는 역할을 하고 있었는데, 그 인덱싱 작업을 매일 아침마다 돌고 있었고 그 대상에는 안타깝게도 s3fs 마운트 폴더도 존재했던 것이다. mlocate가 인덱싱을 위해서 해당 폴더 내 모든 하위 플더들을 순회하면서 인덱싱을 했고, 순회하면서 발생하는 모든 요청들은 모두 s3 서버로의 요청이 되어 network out 지표를 튀게 만들었던 것이다. 메모리 사용량 증가도 자세한 경과까지는 확인하지 못했지만 짧은 시간내에 감당하기 힘든 네트워크 요청들로 인해서 발생한 것이라고 이해하기에 어려움이 없었다.&lt;/p&gt;

&lt;p&gt;조치는 간단했다. mlocate가 인덱싱을 하는 폴더들 중 s3fs 마운트 폴더를 예외로 넣은 후에 s3fs를 재기동해주었다. 메모리 사용량은 정상으로 돌아왔고 며칠이 지난 지금까지도 정상 범주를 벗어나지 않고 있다. network out 그래프 역시 정상의 범주로 돌아왔다.&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Apr 2017 21:00:01 +0900</pubDate>
        <link>http://localhost:4000/troubleshoot/2017/04/25/weird-requests.html</link>
        <guid isPermaLink="true">http://localhost:4000/troubleshoot/2017/04/25/weird-requests.html</guid>
        
        
        <category>troubleshoot</category>
        
      </item>
    
      <item>
        <title>겹겹의 공간들</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;일상에서 우리가 기억이라고 부르는 것도 공간화한 기억이다. 프루스프가 “읽어버신 시간을 찾아서”에서 끊임없이 유년의 마을과 길과 집과 방들을 소환하는 까닭도, 추억이란 게 벌집 같은 공간 속에 특정의 시간들을 압축-공간화하고 있기 때문이다. 바슐라르가 “공감의 시학”에서 한 말처럼 “기억을 생생하게 하는 것은 시간이 아니라 공간이다. 우리들이 오랜 머무름에 의해 구체화된 지속의 아름다운 화석들을 발견하는 것은, 공간에 의해서, 공간 가운데서인 것이다”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그러고보니, 추억이라는 것들을 하나둘씩 들여다보면 시간과 사람들, 그때 했던 생각이나 가졌던 감정들, 표정과 분위기같은 것들은 희끄무레하니 기억이 잘 나지 않는 반면에 공간만큼은 선명하게 기억이 난다. 생각보다 공간이라는 의미는 인간에게 이리도 푸대접받을 만한 작은 의미가 아닌걸까. 우리의 공간에 대한 인식이 이리 뚜렷한 이유가 뭘까.&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Apr 2017 20:00:01 +0900</pubDate>
        <link>http://localhost:4000/daily/2017/04/25/spaces.html</link>
        <guid isPermaLink="true">http://localhost:4000/daily/2017/04/25/spaces.html</guid>
        
        
        <category>daily</category>
        
      </item>
    
      <item>
        <title>이직 면접 이야기</title>
        <description>&lt;p&gt;직장인이 이직 면접을 준비하면서 받는 스트레스가 이혼을 할 때 받는 스트레스와 비슷하다는 이야기를 들은 적이 있다. 글쎄, 내 경우에 비추어보면 조금 과장되다 싶게 느껴지는 이야기긴 하지만, 그만치 감당해야 할 부담이 상당하다는 점에서는 공감을 하게 된다. 면접이 거듭될 때마다 자존감이 손바닥 위 모래처럼 사그러들고, 나는 지금까지 뭐하고 살아왔나 하는 인생 전반에 대한 후회를 하게 되는 것이, 이거 오래 준비하다간 내 명에 못살지 싶었다. 회사 입장에서는 심플하게 &lt;strong&gt;충원하려는 자리에 맞지 않아서&lt;/strong&gt; 퇴짜를 놓았다지만 지원자는 &lt;strong&gt;난 역시 이 정도밖에 안되는 인간인가&lt;/strong&gt; 하는 자괴감을 느끼게 되는 것이다.&lt;/p&gt;

&lt;p&gt;개인적인 경험에 빗대어 봤을 때, 긴장이 풀리면 면접이 잘 풀렸었다. 어느 순간부터 “지금 면접관이 날 어떻게 생각하고 있을까”하는 생각이 머릿 속에서 사라지고, “말빨”이 터져 진짜 내가 누군지를 여실히 떠들고나면 면접관들 입가에서 만족스런 미소를 여볼 수 있었다. 반대로 시작부터 긴장된 상태로 면접에 돌입하고, 지금 내가 긴장을 하고 있다는 사실을 수시로 확인하며 긴장은 더욱 더 증폭이 되고, 이쯤되면 면접관의 간단한 질문도 도저히 한국말로는 안들리게 되는 것이다. 질문을 듣고 고심하는 척 미간을 찌푸리지만 이미 머릿속은 백지다. 그리고 “지금 머릿속이 백지구나”라고 한번 더 되뇌이는 순간 사고는 정지되고 등 뒤로는 식은 땀이 흐른다.&lt;/p&gt;

&lt;p&gt;그래도 면접은 내게 대부분 즐거운 경험이었다. 실패에 대한 리스크가 작지 않다는 점에서 뭔가 위험천만한 스릴 같은 것들이 느껴지기도 했다. 면접을 준비하면서 “나는 이렇게 이런걸 쌓아왔어. 난 괜찮은 놈이야”라고 자기 최면을 걸고, 면접 당일에는 내 자신에 대한 이야기를 한시간에서 두시간 가량 마음껏 늘어놓을 수 있으니 두려움만 떨치면 이건 즐거운 경험이었다. 나는 말이 많은 편은 아니지만 자기 표현에 대한 욕구는 많은 사람이었다. “나는 이런 사람입니다, 이렇게 살아왔구요, 이런 것들을 중요하게 생각합니다.” 하고 떠들고 있다 보면 오랜 친구와 술에 거나하게 취해 인생이 어쩌고 하는 개똥철학을 주고 받는 장면을 떠올리게 한다.&lt;/p&gt;

&lt;p&gt;아쉬운 것은, 신입으로 경력을 쌓고 경력직으로 면접을 준비하게 되면 더이상 내 자신에 대한 의야기 보다는 내가 쌓아온 경험에 대해 이야기해야 하는 시간이 더 많았다. 하지만 이것 또한 나쁘지 않은 것이, 신입 시절 운좋게 내가 좋아할 수 있는 직무를 맡게 되면서 즐겁게 공부하고 경험해온 것들을 떠들 수 있어서 좋았다.&lt;/p&gt;

&lt;p&gt;면접 준비를 하고 면접에 떨어지고 나면 내가 가야할 길들이 좀 더 명확하게 느껴지곤 한다. 내가 가고싶은 회사, 가고싶은 직무의 구직공고에 붙어 있는 기본 요건과 우대 사항들을 보면서 앞으로 내가 공부해 나가야 될 청사진들을 대략적으로 잡을 수 있었고, 면접을 거친 후 내가 대답할 수 있었던 질문과, 대답하지 못했던 질문들을 다시 헤아리며 내가 지금 서있는 위치와 부족한 부분들을 객관적으로 알 수 있었다.&lt;/p&gt;

&lt;p&gt;그렇게 면접 준비와 면접 과정을 거쳐 지금 회사로 이직을 하게 되었고 꽤 만족스럽다고 생각하고 있다. 결과적으로 가고 싶은 회사, 하고 싶은 직무를 맡을 수 있었고, 원하는 조직 문화를 체험하게 됐으며, 똑똑한 사람들과 더 재미있는 일들을 할 수 있게 되었다. 이 회사에 평생 몸담을 수 있다면 그것도 그것 나름대로 좋겠지만 나는 아마 또 이직 준비를 해야 할 시기를 맞게 될 것이다. 하지만 이직을 준비하고 이직을 하게 되는 경험들이 크게 보면 나를 계속해서 채찍질하여 성장하게 만든다고 생각한다.&lt;/p&gt;

</description>
        <pubDate>Thu, 25 Aug 2016 23:40:55 +0900</pubDate>
        <link>http://localhost:4000/daily/2016/08/25/job-interview.html</link>
        <guid isPermaLink="true">http://localhost:4000/daily/2016/08/25/job-interview.html</guid>
        
        
        <category>daily</category>
        
      </item>
    
  </channel>
</rss>
